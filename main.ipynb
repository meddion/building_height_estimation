{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from dataset import BuildingDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from model import get_transform\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_gt_proposals(proposals, gt_boxes):\n",
    "    # type: (List[Tensor], List[Tensor]) -> List[Tensor]\n",
    "    proposals = [\n",
    "        torch.cat((proposal, gt_box)) for proposal, gt_box in zip(proposals, gt_boxes)\n",
    "    ]\n",
    "\n",
    "    return proposals\n",
    "\n",
    "\n",
    "proposals = [torch.rand((1, 4)) for _ in range(3)]\n",
    "gt = [torch.rand((1, 4)) for _ in range(3)]\n",
    "\n",
    "add_gt_proposals(proposals, gt)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for exploration.\n",
    "dataset_test = BuildingDataset(\n",
    "    \"datasets/mlc_training_data/images_annotated/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for exploration.\n",
    "dataset_expl = BuildingDataset(\n",
    "    \"datasets/mlc_training_data/images_annotated/\",\n",
    ")\n",
    "\n",
    "data = [dataset_expl[i] for i in range(10)]\n",
    "# data = [d for d in dataset_expl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, masks, boxes = data[2][0], data[2][1][\"masks\"], data[2][1][\"boxes\"]\n",
    "\n",
    "height_labels = [str(h.item()) for h in data[2][1][\"building_heights\"]]\n",
    "\n",
    "\n",
    "def show_segmentation(img, masks, boxes=None, bcolors=\"red\"):\n",
    "    output_image = draw_segmentation_masks(img, masks.to(torch.bool), alpha=0.8)\n",
    "    if boxes is not None:\n",
    "        output_image = draw_bounding_boxes(\n",
    "            output_image, boxes, height_labels, font_size=15, colors=bcolors\n",
    "        )\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(output_image.permute(1, 2, 0))\n",
    "\n",
    "\n",
    "show_segmentation(img, masks, boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_labels = [d[1][\"building_heights\"] for d in data]\n",
    "height_labels = torch.cat(height_labels).numpy()\n",
    "\n",
    "counts, bins = np.histogram(\n",
    "    height_labels,\n",
    "    bins=[3, 6, 9, 15, 25, 40, 70, 120, 200, max(250, height_labels.max())],\n",
    ")\n",
    "plt.stairs(counts, bins, fill=True)\n",
    "\n",
    "for q in [0.25, 0.5, 0.75, 0.95, 0.99]:\n",
    "    print(f\"Quantile {q}: {np.quantile(height_labels, q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "dataset = BuildingDataset(\n",
    "    \"datasets/mlc_training_data/images_annotated/\",\n",
    "    transforms=get_transform(train=True),\n",
    ")\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=utils.collate_fn,\n",
    ")\n",
    "\n",
    "# For Training\n",
    "images, targets = next(iter(data_loader))\n",
    "\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets)  # Returns losses and detections\n",
    "print(output)\n",
    "\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import train\n",
    "\n",
    "train(\n",
    "    dataset_root=\"datasets/mlc_training_data/images_annotated\",\n",
    "    num_epochs=20,\n",
    "    train_batch_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Engine:\n",
    "    name = \"super shit\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.year = \"2020\"\n",
    "\n",
    "    def start(self):\n",
    "        print(\"engines starts\")\n",
    "\n",
    "    def stop(self):\n",
    "        print(\"engines stops\")\n",
    "\n",
    "\n",
    "class Car:\n",
    "    def __init__(self, engine) -> None:\n",
    "        self.engine = engine\n",
    "\n",
    "    def __getattribute__(self, name: str) -> Any:\n",
    "        if name == \"forward\":\n",
    "            \n",
    "\n",
    "        return getattr(super().__getattribute__(\"engine\"), name)\n",
    "\n",
    "    def stop(self):\n",
    "        print(self.engine.year)\n",
    "        print(self.engine.name)\n",
    "\n",
    "\n",
    "engine = Engine()\n",
    "car = Car(engine)\n",
    "\n",
    "car.stop()\n",
    "car.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bhd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
